# version: "3.8"

# services:
#   ollama:
#     image: ollama/ollama  # Official Ollama Docker image
#     container_name: ollama_server
#     ports:
#       - "11434:11434"  # Expose the Ollama service on port 11434
#     restart: always
#     volumes:
#       - ollama_data:/root/.ollama
#     entrypoint: ["sh", "-c", "ollama serve & sleep 10 && ollama pull mistral && ollama pull gemma3 && wait"]

#   flask_app:
#     build: .
#     container_name: flask_microservice
#     depends_on:
#       - ollama  # Ensure Ollama starts first 
#     environment:
#       - OLLAMA_HOST=ollama_server:11434  # Ensure Flask knows the Ollama server address
#     ports:
#       - "4000:4000"
#     restart: always
#     entrypoint: ["sh", "-c", "until curl -s http://ollama_server:11434; do echo 'Waiting for Ollama to start...'; sleep 5; done; python services/app.py"]

# volumes:
#   ollama_data: