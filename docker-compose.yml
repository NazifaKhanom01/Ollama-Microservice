version: "3.8"

services:
  ollama:
    image: ollama/ollama  # Official Ollama Docker image
    container_name: ollama_server
    ports:
      - "11434:11434"  # Expose the Ollama service on port 11434
    restart: always
    entrypoint: ["sh", "-c", "echo 'Pulling model...' && ollama pull mistral && echo 'Model pulled. Starting Ollama...' && ollama serve --host 0.0.0.0"]
    # Added additional logging to see where it fails

  flask_app:
    build: .
    container_name: flask_microservice
    depends_on:
      - ollama  # Ensure Ollama starts first
    environment:
      - OLLAMA_HOST=ollama_server:11434  # Ensure Flask knows the Ollama server address
    restart: always
    entrypoint: ["sh", "-c", "until curl -s http://ollama_server:11434; do echo 'Waiting for Ollama to start...'; sleep 5; done; python app.py"]
    # Replaced nc with curl to wait for Ollama to be available
